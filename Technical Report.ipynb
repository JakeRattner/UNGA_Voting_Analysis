{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Report: \n",
    "This project seeks to understand how countries cooperate or form groupings during United Nations General Assembly.  More specifically, how do their voting records, each year, reflect attitudes toward major international crises and political decision making.  What resolutions are most significant to forming these groupings and how can they help us understand the major historical events of the time.\n",
    "\n",
    "In order to do so, I looked at the entire General Assembly voting record of all countries from 1946 - 2017. I've chosen not to include the voting records of the UN Security Council because of the limit number of members and voting history.\n",
    "\n",
    "## Data Acquisition\n",
    "The data comes from the UN API portal here: http://data.un.org/Host.aspx?Content=API maintained by the United Nations. The specific data I used is organized by Erik Voeten from Harvard Univerity an is available here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/12379. The data includes a large amount of information not needed for the analysis.  In total there are 1,099,156 rows including votes on 5,598 resolutions.\n",
    "   \n",
    "## Data Cleaning and Feature Engineering\n",
    "The data is very clean.  It doesn't include nulls, missing data or any important typos.  The was some cleaning and feature engineering due to unnecessary data as well as a need to reformat and engineer the data for use in modeling.\n",
    "\n",
    "I first removed all records for non-member country voting records (typically coded as \"present\"). These states, when coded as non-members, are included in the data set for consistency.  They had to be removed because they were not able to vote as a non-member and may not have even been nations at the time of voting.  I only wanted to look at 'yes' and 'no, votes so all other vote types were dropped.  With only yes and no votes remaining I changed the coding of yes and no votes to 1 and 0 for processing.  \n",
    "\n",
    "I then create a library of years and built a function to create a dataframe for each year of the approx. 70 years of voting history so that years could be analyzed separately.  \n",
    "\n",
    "For PCA the dataframes for each year are joined with a dataset of resolution descriptions and merged with voting records so that the specifics of each resolution could be visualized if they were important.\n",
    "\n",
    "   \n",
    "## Modeling Process\n",
    "\n",
    "I used HDBSCAN as the basis for this project.  -means and DBSCAN were evaluated, but HDBSCAN performed best in terms of both performance (measure by silhouette score) and in automatically selected the number of  clusters.  I chose a minimum cluster size of 10.  Some years still create errors due to  failure to cluster with that year's information, but min clusters of 10 was the size that created the most consistent clustering over the period the data was collected (1946 - 2017).  The model was found to be successful when it repeatedly produced clusters consistent with historical literature and created a low number of errors when applied to each year.\n",
    "\n",
    "I then passed my data through a Principal Component Analysis (PCA).  This proved useful for feature selection and visualization. PCA served two purposes in this project.  First, it allows the clusters to be visualized in 2D.  Once the model was constructed it proved successful right away.  It shows the size and the arrangement of the yearly clustering.  There are some limitations to the visualization however.  You can see closeness in the clusters, but its difficult to interpret this.  It also doesn't show the geographic layout.  At the moment I'm not able to show which country in the visualization is which.\n",
    "\n",
    "Second, I'm using the PCA analysis to find which resolutions were weighted most heavily.  In other words which resolutions contributed the most variance and therefore would have the greatest effect on clusters in a given year.  This was possible because virtually all the variance was placed on component 1 once the analysis was run.\n",
    "\n",
    "   \n",
    "## Future deployment strategies, additions of data, or modeling techniques\n",
    "\n",
    "There's a quite a few useful strategies and techniques that can be employed in the future. I would consider include abstaining votes in the model (weighted by a factor based on literature review).  It would also be useful to categorize resolutions based on NLP Topic Modeling. Topics could be used to to better understand clustering around certain issues. I could conceivably rerun models based on specific topic areas, and possibly run clusters over 5-year periods to see long term topic-based trends.\n",
    "\n",
    "It would also be very useful to visualize clusters geographically by placing them on a world map\n",
    "Finally, I would also like to update the output function so that, as the number of member nations increases, the minimum cluster size increases in proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
